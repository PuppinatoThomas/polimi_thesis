Of course. Data wrangling (also known as data munging) is the process of transforming and mapping raw data into a clean, structured, and usable format for analysis. It's a crucial and often time-consuming step in the data science pipeline.

The operations required can be broken down into a logical workflow. Here is a comprehensive list of data wrangling operations, categorized by their purpose.

### Phase 1: Discovery & Structuring

This is the initial phase where you understand your data and get it into a workable structure.

1.  **Data Acquisition & Loading:** The very first step is to acquire and read the data into your analysis environment (e.g., Python/pandas, R, SQL).
    *   **Operations:** Reading from various sources like CSV, Excel, JSON, XML, databases (SQL), web APIs, Parquet files, and web scraping.

2.  **Data Assessment & Profiling:** Understand what you're working with before making changes.
    *   **Operations:**
        *   View the first/last few rows (`df.head()`, `df.tail()`).
        *   Check dimensions (number of rows and columns).
        *   Get summary statistics (`df.describe()`).
        *   Check data types of each column (`df.info()`, `df.dtypes`).
        *   Identify missing values.
        *   Check for duplicate rows.

3.  **Data Structuring:** Raw data is often not in a "tidy" format. Structuring involves reshaping it.
    *   **Operations:**
        *   **Reshaping (Pivoting/Melting):** Converting data from wide to long format (`melt`) or long to wide format (`pivot`).
        *   **Transposing:** Swapping rows and columns.
        *   **Column Splitting/Combining:** Splitting a single column (e.g., "Full Name") into multiple columns ("First Name", "Last Name") or combining the reverse.

### Phase 2: Cleaning

This is the core of data wrangling, where you fix quality issues.

4.  **Handling Missing Data:** Deciding what to do with null or empty values.
    *   **Operations:**
        *   **Deletion:** Removing rows or columns with missing values (`dropna()`). Use with caution.
        *   **Imputation:** Filling missing values with a statistic (mean, median, mode) or using more advanced methods (forward-fill, model-based imputation) (`fillna()`).

5.  **Handling Duplicates:** Removing redundant data points.
    *   **Operations:** Identifying and dropping exact or fuzzy duplicates (`drop_duplicates()`).

6.  **Type Conversion:** Ensuring each column has the correct data type.
    *   **Operations:** Converting strings to dates/times (`to_datetime()`), strings to numeric (`to_numeric()`), integers to floats, objects to categories, etc.

7.  **String Cleaning & Standardization:** Fixing inconsistencies in text data.
    *   **Operations:**
        *   Changing case (upper, lower, title).
        *   Removing leading/trailing whitespace (`strip()`).
        *   Replacing substrings or characters.
        *   Using regular expressions for complex pattern matching and extraction.

8.  **Handling Outliers:** Identifying and addressing anomalous data points that could skew analysis.
    *   **Operations:**
        *   **Identification:** Using statistical methods (IQR, Z-scores) or visualization (box plots).
        *   **Treatment:** Capping, transforming, or removing outliers based on domain knowledge.

### Phase 3: Enrichment & Transformation

This phase involves creating new features or modifying existing ones to make the data more useful.

9.  **Feature Engineering:** Creating new columns (features) from existing ones to provide more signal to a model.
    *   **Operations:**
        *   **Binning/Discretization:** Converting continuous variables into categorical ranges (e.g., age groups).
        *   **Creating Dummy Variables:** Converting categorical variables into binary (0/1) columns for machine learning (One-Hot Encoding, `get_dummies()`).
        *   **Mathematical Transformations:** Applying log, square root, or power transformations.
        *   **Date/Time Decomposition:** Extracting parts of a date like year, month, day, day-of-week, hour, etc.

10. **Scaling & Normalization:** Adjusting the range of numerical features so they are comparable.
    *   **Operations:** Min-Max Scaling, Standardization (Z-score), Robust Scaling.

11. **Combining Datasets:** Merging multiple data sources.
    *   **Operations:**
        *   **Joining/Merging:** Combining datasets based on a common key (like SQL joins: inner, left, right, outer) (`merge()`, `join()`).
        *   **Concatenation:** Appending rows or columns from another dataset (`concat()`).

### Phase 4: Validation & Publishing

The final steps to ensure data quality and prepare it for use.

12. **Data Validation:** Writing scripts or assertions to verify that the data meets certain quality rules after cleaning.
    *   **Operations:**
        *   Checking data types are correct.
        *   Ensuring values fall within an expected range.
        *   Verifying that key columns have no nulls.
        *   Validating unique constraints.

13. **Exporting (Publishing):** Writing the cleaned and transformed dataset to a new file or database.
    *   **Operations:** Saving to formats like CSV, Excel, Parquet, or writing to a SQL database.

---

### Summary Table of Key Operations

| Category | Operation | Description | Common Tools/Functions |
| :--- | :--- | :--- | :--- |
| **Discovery** | **Profiling** | Understand structure, stats, and quality. | `.head()`, `.describe()`, `.info()` |
| **Structuring** | **Reshaping** | Pivot from long to wide format or vice-versa. | `.pivot()`, `.melt()` |
| **Cleaning** | **Handling Missing Data** | Remove or fill null values. | `.dropna()`, `.fillna()` |
| **Cleaning** | **Type Conversion** | Convert data types (e.g., string to date). | `.astype()`, `pd.to_datetime()` |
| **Cleaning** | **String Manipulation** | Clean and standardize text data. | `.str.strip()`, `.str.replace()`, Regex |
| **Transformation**| **Feature Engineering** | Create new columns from existing ones. | Binning, `np.where()`, DateTime decomposition |
| **Transformation**| **Handling Duplicates** | Identify and remove duplicate rows. | `.duplicated()`, `.drop_duplicates()` |
| **Transformation**| **Scaling/Normalization** | Standardize the range of numerical features. | `StandardScaler`, `MinMaxScaler` |
| **Combining** | **Joining/Merging** | Combine datasets based on a key. | `.merge()`, `pd.merge()`, `JOIN` (SQL) |
| **Validation** | **Exporting** | Save the cleaned data to a file. | `.to_csv()`, `.to_parquet()`, `.to_sql()` |

**Important Note:** Data wrangling is a highly **iterative process**. You will often loop back to earlier steps (like discovery and assessment) after performing a transformation to ensure it had the intended effect.
